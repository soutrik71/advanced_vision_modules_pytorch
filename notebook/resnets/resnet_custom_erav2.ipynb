{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run/code/Users/soutrik.chowdhury/advanced_vision_modules_pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run/code/Users/soutrik.chowdhury/advanced_vision_modules_pytorch\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will design a custom resnet architecture as specified in session 10 ot ERA course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torcheval.metrics import MulticlassAccuracy, BinaryAccuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import (\n",
    "    OneCycleLR,\n",
    "    StepLR,\n",
    "    ExponentialLR,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(\n",
    "    train_loss,\n",
    "    val_loss,\n",
    "    train_acc,\n",
    "    val_acc,\n",
    "    labels,\n",
    "    colors,\n",
    "    loss_legend_loc=\"upper center\",\n",
    "    acc_legend_loc=\"upper left\",\n",
    "    legend_font=5,\n",
    "    fig_size=(16, 10),\n",
    "    sub_plot1=(1, 2, 1),\n",
    "    sub_plot2=(1, 2, 2),\n",
    "):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.figure\n",
    "\n",
    "    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n",
    "\n",
    "    for i in range(len(train_loss)):\n",
    "        x_train = range(len(train_loss[i]))\n",
    "        x_val = range(len(val_loss[i]))\n",
    "\n",
    "        min_train_loss = np.array(train_loss[i]).min()\n",
    "\n",
    "        min_val_loss = np.array(val_loss[i]).min()\n",
    "\n",
    "        plt.plot(\n",
    "            x_train,\n",
    "            train_loss[i],\n",
    "            linestyle=\"-\",\n",
    "            color=\"tab:{}\".format(colors[i]),\n",
    "            label=\"TRAIN ({0:.4}): {1}\".format(min_train_loss, labels[i]),\n",
    "        )\n",
    "        plt.plot(\n",
    "            x_val,\n",
    "            val_loss[i],\n",
    "            linestyle=\"--\",\n",
    "            color=\"tab:{}\".format(colors[i]),\n",
    "            label=\"VALID ({0:.4}): {1}\".format(min_val_loss, labels[i]),\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"epoch no.\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend(loc=loss_legend_loc, prop={\"size\": legend_font})\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n",
    "\n",
    "    for i in range(len(train_acc)):\n",
    "        x_train = range(len(train_acc[i]))\n",
    "        x_val = range(len(val_acc[i]))\n",
    "\n",
    "        max_train_acc = np.array(train_acc[i]).max()\n",
    "\n",
    "        max_val_acc = np.array(val_acc[i]).max()\n",
    "\n",
    "        plt.plot(\n",
    "            x_train,\n",
    "            train_acc[i],\n",
    "            linestyle=\"-\",\n",
    "            color=\"tab:{}\".format(colors[i]),\n",
    "            label=\"TRAIN ({0:.4}): {1}\".format(max_train_acc, labels[i]),\n",
    "        )\n",
    "        plt.plot(\n",
    "            x_val,\n",
    "            val_acc[i],\n",
    "            linestyle=\"--\",\n",
    "            color=\"tab:{}\".format(colors[i]),\n",
    "            label=\"VALID ({0:.4}): {1}\".format(max_val_acc, labels[i]),\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"epoch no.\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc=acc_legend_loc, prop={\"size\": legend_font})\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        delta=1e-5,\n",
    "        trace_func=print,\n",
    "        path=\"models\",\n",
    "        model_name=\"model.pt\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(\n",
    "                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "            )\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(self.path, self.model_name))\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get device (if GPU is available, use GPU)\"\"\"\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed since nn.Parameter are randomly initialzied\n",
    "set_seed(42)\n",
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = get_device()\n",
    "print(device)\n",
    "batch_size = 512\n",
    "epochs = 25\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the image classes\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for the dataset using Compose from albumentations\n",
    "def data_augmentations():\n",
    "    \"\"\"Data Augmentations for the CIFAR10 dataset\"\"\"\n",
    "    train_transforms = alb.Compose(\n",
    "        [\n",
    "            alb.Resize(\n",
    "                height=36, width=36, always_apply=True, interpolation=cv2.INTER_NEAREST\n",
    "            ),\n",
    "            alb.RandomCrop(height=32, width=32, always_apply=True),\n",
    "            alb.Flip(p=0.5),\n",
    "            alb.CoarseDropout(\n",
    "                max_holes=1,\n",
    "                max_height=8,\n",
    "                max_width=8,\n",
    "                min_holes=1,\n",
    "                min_height=8,\n",
    "                min_width=8,\n",
    "                fill_value=[0.4914, 0.4822, 0.4465],\n",
    "                mask_fill_value=None,\n",
    "            ),\n",
    "            alb.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = alb.Compose(\n",
    "        [\n",
    "            alb.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    return train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading train and test data\n",
    "train = datasets.CIFAR10(\"./data\", train=True, download=True)\n",
    "test = datasets.CIFAR10(\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDS(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms, test_transforms = data_augmentations()\n",
    "train_dateset = CifarDS(train.data, train.targets, train_transforms)\n",
    "test_dateset = CifarDS(test.data, test.targets, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(\n",
    "    train_dateset, test_dateset, train_transforms, test_transforms, dataloader_args\n",
    "):\n",
    "    \"\"\"Data loader for the CIFAR10 dataset\"\"\"\n",
    "    # Loading custom datasets\n",
    "    train_dateset = CifarDS(train_dateset.data, train_dateset.targets, train_transforms)\n",
    "    test_dateset = CifarDS(test_dateset.data, test_dateset.targets, test_transforms)\n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dateset, **dataloader_args)\n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(test_dateset, **dataloader_args)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data,label in train_dateset:\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    print(data.mean())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "dataloader_args = (\n",
    "    dict(shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    if device.type == \"cuda\"\n",
    "    else dict(shuffle=True, batch_size=batch_size)\n",
    ")\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dateset, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test_dateset, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "    ):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.resnet_block1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=(2, 2),\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                stride=(1, 1),\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass of the resnet block\"\"\"\n",
    "        interm_op = self.conv_layer1(x)\n",
    "        residual_op = self.resnet_block1(x)\n",
    "        # print(interm_op.shape)\n",
    "        # print(residual_op.shape)\n",
    "        op = residual_op + interm_op\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_block = ResnetBlock(64,128).to(device)\n",
    "summary(resnet_block, input_size=(64, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ResnetNetwork, self).__init__()\n",
    "        # PrepLayer - Conv 3x3 s1, p1) >> BN >> RELU [64k]\n",
    "        self.presentation_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.resnet_layer1 = ResnetBlock(in_channels=64, out_channels=128)\n",
    "\n",
    "        self.norm_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.resnet_layer2 = ResnetBlock(in_channels=256, out_channels=512)\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(4, 4))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass of the resnet network\"\"\"\n",
    "        op = self.presentation_layer(x)\n",
    "        op = self.resnet_layer1(op)\n",
    "        op = self.norm_conv1(op)\n",
    "        op = self.resnet_layer2(op)\n",
    "        op = self.pooling(op)\n",
    "        op = self.classifier(op)\n",
    "        return F.log_softmax(op, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResnetNetwork(in_channels=3, num_classes=10).to(device)\n",
    "summary(resnet_model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated training module---\n",
    "def train_module(\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    metric,\n",
    "    train_losses: list,\n",
    "    train_metrics: list,\n",
    "):\n",
    "\n",
    "    # setting model to train mode\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "\n",
    "    # batch metrics\n",
    "    train_loss = 0\n",
    "    train_metric = 0\n",
    "    processed_batch = 0\n",
    "\n",
    "    for idx, (data, label) in enumerate(pbar):\n",
    "        # setting up device\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # forward pass output\n",
    "        preds = model(data)\n",
    "\n",
    "        # calc loss\n",
    "        loss = criterion(preds, label)\n",
    "        train_loss += loss.item()\n",
    "        # print(f\"training loss for batch {idx} is {loss}\")\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()  # flush out  existing grads\n",
    "        loss.backward()  # back prop of weights wrt loss\n",
    "        optimizer.step()  # optimizer step -> minima\n",
    "\n",
    "        # metric calc\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        # print(f\"preds:: {preds}\")\n",
    "        metric.update(preds, label)\n",
    "        train_metric += metric.compute().detach().item()\n",
    "\n",
    "        # updating batch count\n",
    "        processed_batch += 1\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Avg Train Loss: {train_loss/processed_batch} Avg Train Metric: {train_metric/processed_batch}\"\n",
    "        )\n",
    "\n",
    "    # It's typically called after the epoch completes\n",
    "    metric.reset()\n",
    "    # updating epoch metrics\n",
    "    train_losses.append(train_loss / processed_batch)\n",
    "    train_metrics.append(train_metric / processed_batch)\n",
    "\n",
    "    return train_losses, train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated testing modules---\n",
    "def test_module(\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    metric,\n",
    "    test_losses,\n",
    "    test_metrics,\n",
    "):\n",
    "    # setting model to eval mode\n",
    "    model.eval()\n",
    "    pbar = tqdm(test_dataloader)\n",
    "\n",
    "    # batch metrics\n",
    "    test_loss = 0\n",
    "    test_metric = 0\n",
    "    processed_batch = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for idx, (data, label) in enumerate(pbar):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            # predictions\n",
    "            preds = model(data)\n",
    "            # print(preds.shape)\n",
    "            # print(label.shape)\n",
    "\n",
    "            # loss calc\n",
    "            loss = criterion(preds, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # metric calc\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            metric.update(preds, label)\n",
    "            test_metric += metric.compute().detach().item()\n",
    "\n",
    "            # updating batch count\n",
    "            processed_batch += 1\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Avg Test Loss: {test_loss/processed_batch} Avg Test Metric: {test_metric/processed_batch}\"\n",
    "            )\n",
    "\n",
    "        # It's typically called after the epoch completes\n",
    "        metric.reset()\n",
    "        # updating epoch metrics\n",
    "        test_losses.append(test_loss / processed_batch)\n",
    "        test_metrics.append(test_metric / processed_batch)\n",
    "\n",
    "    return test_losses, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver setup----\n",
    "# optmizer\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=learning_rate)\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metric\n",
    "metric = MulticlassAccuracy(device=device, num_classes=10)\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=5, verbose=True, model_name=\"resnet_base_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_metrics = []\n",
    "test_losses = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_losses, train_metrics = train_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        metric,\n",
    "        train_losses,\n",
    "        train_metrics,\n",
    "    )\n",
    "    test_losses, test_metrics = test_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        metric,\n",
    "        test_losses,\n",
    "        test_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "    early_stopping(\n",
    "        test_losses[-1], resnet_model, epoch\n",
    "    )  # last recorded test loss to measure the improvement against the prior one\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 11))\n",
    "plot_loss_accuracy(\n",
    "    train_loss=[train_losses],\n",
    "    val_loss=[test_losses],\n",
    "    train_acc=[train_metrics],\n",
    "    val_acc=[test_metrics],\n",
    "    labels=[\"base_resnet\"],\n",
    "    colors=[\"blue\"],\n",
    "    loss_legend_loc=\"upper left\",\n",
    "    acc_legend_loc=\"upper left\",\n",
    "    legend_font=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The training accuracy has been around 91% where as we validation accuracy around 86%\n",
    "* This could have been fixed using a couple of techniques like adding image augmentation , different weight init or adding regularization which all we will try later\n",
    "* Next we will try to find a sweet value of LR using LR finder and then use cyclic LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redifining the model and drivers for Cyclic LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResnetNetwork(in_channels=3, num_classes=10).to(device)\n",
    "summary(resnet_model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_drivers(\n",
    "    model: nn.Module,\n",
    "    learning_rate: float,\n",
    "    num_classes: int,\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Initialize drivers for training\"\"\"\n",
    "    # optmizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # metric\n",
    "    metric = MulticlassAccuracy(device=device, num_classes=num_classes)\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, model_name=model_name)\n",
    "\n",
    "    return model, optimizer, criterion, metric, early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optmizer\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=1e-5)\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metric\n",
    "metric = MulticlassAccuracy(device=device, num_classes=10)\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=5, verbose=True, model_name=\"resnet_base_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggested_lr(model, optimizer, criterion, device, end_lr, num_iter, train_loader):\n",
    "    \"\"\"Suggested learning rate\"\"\"\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "    lr_finder.range_test(\n",
    "        train_loader, end_lr=end_lr, num_iter=num_iter, step_mode=\"linear\"\n",
    "    )\n",
    "    lr_finder.plot()\n",
    "    lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_lr = 5.23E-03\n",
    "print(suggested_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a LR scheduler accordingly\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=suggested_lr,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    anneal_strategy=\"linear\",\n",
    "    pct_start=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_metrics = []\n",
    "test_losses = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_losses, train_metrics = train_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        metric,\n",
    "        train_losses,\n",
    "        train_metrics,\n",
    "    )\n",
    "    test_losses, test_metrics = test_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        metric,\n",
    "        test_losses,\n",
    "        test_metrics,\n",
    "    )\n",
    "    # Lr scheduler added\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate ahead: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    early_stopping(\n",
    "        test_losses[-1], resnet_model, epoch\n",
    "    )  # last recorded test loss to measure the improvement against the prior one\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets use Step LR and ExponentialLR to check it we can improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResnetNetwork(in_channels=3, num_classes=10).to(device)\n",
    "summary(resnet_model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optmizer\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=suggested_lr)\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metric\n",
    "metric = MulticlassAccuracy(device=device, num_classes=10)\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=5, verbose=True, model_name=\"resnet_base_model.pt\"\n",
    ")\n",
    "# Step LR\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.95)  # decreae by 5 % after every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_metrics = []\n",
    "test_losses = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_losses, train_metrics = train_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        metric,\n",
    "        train_losses,\n",
    "        train_metrics,\n",
    "    )\n",
    "    test_losses, test_metrics = test_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        metric,\n",
    "        test_losses,\n",
    "        test_metrics,\n",
    "    )\n",
    "    # Lr scheduler added\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate ahead: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    early_stopping(\n",
    "        test_losses[-1], resnet_model, epoch\n",
    "    )  # last recorded test loss to measure the improvement against the prior one\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResnetNetwork(in_channels=3, num_classes=10).to(device)\n",
    "summary(resnet_model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optmizer\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=suggested_lr)\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metric\n",
    "metric = MulticlassAccuracy(device=device, num_classes=10)\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=5, verbose=True, model_name=\"resnet_base_model.pt\"\n",
    ")\n",
    "# Step LR\n",
    "scheduler = ExponentialLR( optimizer, gamma=0.95, verbose=True)  # exponential decay by 5 % after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_metrics = []\n",
    "test_losses = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_losses, train_metrics = train_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        metric,\n",
    "        train_losses,\n",
    "        train_metrics,\n",
    "    )\n",
    "    test_losses, test_metrics = test_module(\n",
    "        resnet_model,\n",
    "        device,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        metric,\n",
    "        test_losses,\n",
    "        test_metrics,\n",
    "    )\n",
    "    # Lr scheduler added\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate ahead: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    early_stopping(\n",
    "        test_losses[-1], resnet_model, epoch\n",
    "    )  # last recorded test loss to measure the improvement against the prior one\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 11))\n",
    "plot_loss_accuracy(\n",
    "    train_loss=[train_losses],\n",
    "    val_loss=[test_losses],\n",
    "    train_acc=[train_metrics],\n",
    "    val_acc=[test_metrics],\n",
    "    labels=[\"resnet_expLR\"],\n",
    "    colors=[\"blue\"],\n",
    "    loss_legend_loc=\"upper left\",\n",
    "    acc_legend_loc=\"upper left\",\n",
    "    legend_font=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(type, *args, **kwargs):\n",
    "    if type == \"StepLR\":\n",
    "        return StepLR(*args, **kwargs)\n",
    "    elif type == \"ExponentialLR\":\n",
    "        return ExponentialLR(*args, **kwargs)\n",
    "\n",
    "    elif type == \"CyclicLR\":\n",
    "        return OneCycleLR(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will complete the session 10 assigment in modular fashion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
