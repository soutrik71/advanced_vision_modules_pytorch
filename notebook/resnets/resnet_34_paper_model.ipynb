{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run/code/Users/soutrik.chowdhury/advanced_vision_modules_pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run/code/Users/soutrik.chowdhury/advanced_vision_modules_pytorch\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "* Application of Resnet-34 paper model as described in article: https://python.plainenglish.io/paper-walkthrough-residual-network-resnet-62af58d1c521\n",
    "* We will test the model on random sample which represtents the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://debuggercafe.com/wp-content/uploads/2021/04/diff_dimensions_residual_block.jpg\n",
    "\n",
    "https://debuggercafe.com/wp-content/uploads/2021/04/diff_dimensions_residual_block.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "random_tensor_batch = torch.randn(16,3,32,32, dtype=torch.float32, device='cuda')\n",
    "print(random_tensor_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual block:\n",
    "* ResNet-34, the residual block comprises of two convolution layers, both using the kernel size of 3×3, stride of 1, and padding of 1 along with Batchnorm and Relu\\\n",
    "* Provision for the skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    \"\"\"This block contains the structure of a residual block without the downsampling of input but with skip connection for the Input layer\"\"\"\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super(Residual_Block, self).__init__()\n",
    "        # 1st convolution layer for feature extraction\n",
    "        self.conv_residual1 = nn.Conv2d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn_residual1 = nn.BatchNorm2d(num_channels)\n",
    "        # 2nd convolution layer for feature extraction\n",
    "        self.conv_residual2 = nn.Conv2d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn_residual2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Residual block forward pass whose flow is as follows:\n",
    "        Passing through the first cnn block having 3*3 kernel size, stride of 1 and padding of 1\n",
    "        Batchnorm and relu\n",
    "        Passing through the second cnn block having 3*3 kernel size, stride of 1 and padding of 1\n",
    "        Batchnorm\n",
    "        Adding the skip connection\n",
    "        Relu application\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "        out = self.conv_residual1(x)\n",
    "        out = self.bn_residual1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv_residual2(out)\n",
    "        out = self.bn_residual2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Residual_Block                           [1, 3, 32, 32]            --\n",
       "├─Conv2d: 1-1                            [1, 3, 32, 32]            81\n",
       "├─BatchNorm2d: 1-2                       [1, 3, 32, 32]            6\n",
       "├─ReLU: 1-3                              [1, 3, 32, 32]            --\n",
       "├─Conv2d: 1-4                            [1, 3, 32, 32]            81\n",
       "├─BatchNorm2d: 1-5                       [1, 3, 32, 32]            6\n",
       "├─ReLU: 1-6                              [1, 3, 32, 32]            --\n",
       "==========================================================================================\n",
       "Total params: 174\n",
       "Trainable params: 174\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.17\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.11\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "residual_block_test = Residual_Block(num_channels=3).to(device)\n",
    "summary(residual_block_test, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = residual_block_test(random_tensor_batch)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Block:\n",
    "* https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-Itl6-5b-8ODVHbm61n4vg.png\n",
    "* this section is going to focus on the residual blocks that use dashed curve to denote its skip connection\n",
    "* The convolution layer fundamentally remian the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block_Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"This block contains the structure of a residual block with downsampling of input and skip connection for the Input layer\"\"\"\n",
    "        super(Residual_Block_Transition, self).__init__()\n",
    "\n",
    "        # This is the kind of maxpooling layer for the Input interms of sie reduction as strid is 2,2\n",
    "        self.conv_identity_adapt = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(2, 2),\n",
    "            padding=(0, 0),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn_identity_adapt = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Application of a 3*3 kernel size, stride of 2 and padding of 1 which again reduces the size of the input along with feature extraction\n",
    "        self.conv_transition1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn_transition1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # this is a normal kerel size, stride of 1 and padding of 1 which is just meant for feature extraction\n",
    "        self.conv_transition2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn_transition2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Residual block forward pass whose flow is as follows:\n",
    "        Create a skip connect with reduced dims for future addition\n",
    "        First cnn block having 3*3 kernel size, stride of 2 and padding of 1 which makes it input reducer plus feature extractor\n",
    "        Batchnorm and relu\n",
    "        Second cnn block having 3*3 kernel size, stride of 1 and padding of 1 which makes it feature extractor\n",
    "        Batchnorm\n",
    "        Adding the skip connection\n",
    "        Relu application\n",
    "        \"\"\"\n",
    "        identity = self.bn_identity_adapt(self.conv_identity_adapt(x))\n",
    "        out = self.relu(self.bn_transition1(self.conv_transition1(x)))\n",
    "        out = self.relu(self.bn_transition2(self.conv_transition2(out)))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Residual_Block_Transition                [1, 16, 16, 16]           --\n",
       "├─Conv2d: 1-1                            [1, 16, 16, 16]           48\n",
       "├─BatchNorm2d: 1-2                       [1, 16, 16, 16]           32\n",
       "├─Conv2d: 1-3                            [1, 16, 16, 16]           432\n",
       "├─BatchNorm2d: 1-4                       [1, 16, 16, 16]           32\n",
       "├─ReLU: 1-5                              [1, 16, 16, 16]           --\n",
       "├─Conv2d: 1-6                            [1, 16, 16, 16]           2,304\n",
       "├─BatchNorm2d: 1-7                       [1, 16, 16, 16]           32\n",
       "├─ReLU: 1-8                              [1, 16, 16, 16]           --\n",
       "├─ReLU: 1-9                              [1, 16, 16, 16]           --\n",
       "==========================================================================================\n",
       "Total params: 2,880\n",
       "Trainable params: 2,880\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.71\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.20\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "residual_block_transition = Residual_Block_Transition(\n",
    "    in_channels=3, out_channels=16\n",
    ").to(device)\n",
    "summary(residual_block_transition, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we also need to ensure that the skip connection to have the exact same size as the output. This is simply because element-wise summation is not possible to be done if the dimension of two tensors is different. In order to do so, we initialize a layer called self.conv_identity_adapt in which it is actually similar to self.conv_transition1 yet with the kernel size of 1×1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done is to\n",
    "* Residual_Block is specialized to process a tensor in which the input and output dimension is exactly the same.\n",
    "* Residual_Block_Trans, on the other hand, is going to make the output tensor to be spatially twice as small as the input (using stride=2, as shown at line #(1) and #(2)), while the number of channels will be twice larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Resnet Image\n",
    "\n",
    "https://miro.medium.com/v2/resize:fit:4604/format:webp/1*03YmxWgvuQgZluZHZjZuVg.png\n",
    "\n",
    "First Layer Non_resnet:\n",
    "* The inital part has a structure of Application of Convolution of (7,7) with stride =2  and padding of 3 which basically halfs the input size\n",
    "* Then we application of batchnorn and relu and maxpool with (3,3) stride of (2,2) and padding of (1,1)\n",
    "* The size of those kernels themselves are 7×7. In this first conv layer, we set the stride and the padding to (2,2) and (3,3), respectively, in order to make the spatial output dimension to be 112×112. Technically speaking, we actually don’t need to specify the image size to the network as it is going to work adaptively. This means that if you feed this part of network with 100×100 image, then the output size is going to be 50×50. We can also see in the Codeblock 9 above that max-pooling layer is implemented with the stride of (2,2), which causes the image size to be reduced by twice again. Thus, up until this point our image is going to be four times smaller than its original size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_repeats, in_channels, num_classes, first_out_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_repeats (int): Number of repetitions of the ResNet block\n",
    "            in_channels (int): Number of input channels\n",
    "            num_classes (int): Number of classes\n",
    "            first_out_channels (int): Number of output channels for the first convolution layer\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # first layer non_resnet\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=first_out_channels,\n",
    "            kernel_size=(7, 7),\n",
    "            stride=(2, 2),\n",
    "            padding=(3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(first_out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "        # first resnet layer with residual blocks conv2x\n",
    "        self.residual_block_conv2_x = nn.ModuleList()\n",
    "        for _ in range(num_repeats[0]):\n",
    "            self.residual_block_conv2_x.append(\n",
    "                Residual_Block(num_channels=first_out_channels)\n",
    "            )\n",
    "\n",
    "        # Using Residual_Block_Trans to connect conv2_x and conv3_x; in_channels set to 64 and out_channels to 128 (line #(1) Codeblock 11)\n",
    "        # then creating and stacking all blocks for conv3_x (#(2)).\n",
    "\n",
    "        # conv2x which increases from 64-> 128\n",
    "        self.residual_block_trans3x = Residual_Block_Transition(\n",
    "            in_channels=64, out_channels=128\n",
    "        )  # this is the dotted transition block\n",
    "\n",
    "        self.residual_blocks_conv3_x = nn.ModuleList()\n",
    "        for _ in range(\n",
    "            num_repeats[1] - 1\n",
    "        ):  # -1 as we have n-1 residual blocks and one transition block\n",
    "            self.residual_blocks_conv3_x.append(Residual_Block(num_channels=128))\n",
    "\n",
    "        # 3 We use the same technique to connect conv3_x and conv4_x\n",
    "\n",
    "        # conv3x which increases from 128-> 256\n",
    "        self.residual_block_trans4 = Residual_Block_Transition(\n",
    "            in_channels=128, out_channels=256\n",
    "        )\n",
    "\n",
    "        self.residual_blocks_conv4_x = nn.ModuleList()\n",
    "        for _ in range(num_repeats[2] - 1):\n",
    "            self.residual_blocks_conv4_x.append(Residual_Block(num_channels=256))\n",
    "\n",
    "        # conv4x which increases from 256-> 512\n",
    "        self.residual_block_trans5 = Residual_Block_Transition(\n",
    "            in_channels=256, out_channels=512\n",
    "        )\n",
    "\n",
    "        self.residual_blocks_conv5_x = nn.ModuleList()\n",
    "        for _ in range(num_repeats[3] - 1):\n",
    "            self.residual_blocks_conv5_x.append(Residual_Block(num_channels=512))\n",
    "\n",
    "        # Final classification layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward function for classification layer\"\"\"\n",
    "        # presentation layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # first residual conv block\n",
    "        for layer in self.residual_block_conv2_x:\n",
    "            x = layer(x)\n",
    "\n",
    "        # first transition block to increase the channel size and reduce the image size\n",
    "        x = self.residual_block_trans3x(x)\n",
    "\n",
    "        # second residual block\n",
    "        for layer in self.residual_blocks_conv3_x:\n",
    "            x = layer(x)\n",
    "\n",
    "        # second transition block to increase the channel size and reduce the image size\n",
    "        x = self.residual_block_trans4(x)\n",
    "\n",
    "        # third residual block\n",
    "        for layer in self.residual_blocks_conv4_x:\n",
    "            x = layer(x)\n",
    "\n",
    "        # third transition block to increase the channel size and reduce the image size\n",
    "        x = self.residual_block_trans5(x)\n",
    "\n",
    "        # fourth residual block\n",
    "        for layer in self.residual_blocks_conv5_x:\n",
    "            x = layer(x)\n",
    "\n",
    "        # final classification layer\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 16, 16]           9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 16, 16]           128\n",
       "├─ReLU: 1-3                              [1, 64, 16, 16]           --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 8, 8]             --\n",
       "├─ModuleList: 1-5                        --                        --\n",
       "│    └─Residual_Block: 2-1               [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 8, 8]             --\n",
       "│    └─Residual_Block: 2-2               [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 8, 8]             --\n",
       "│    └─Residual_Block: 2-3               [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-15                   [1, 64, 8, 8]             --\n",
       "│    │    └─Conv2d: 3-16                 [1, 64, 8, 8]             36,864\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 64, 8, 8]             128\n",
       "│    │    └─ReLU: 3-18                   [1, 64, 8, 8]             --\n",
       "├─Residual_Block_Transition: 1-6         [1, 128, 4, 4]            --\n",
       "│    └─Conv2d: 2-4                       [1, 128, 4, 4]            8,192\n",
       "│    └─BatchNorm2d: 2-5                  [1, 128, 4, 4]            256\n",
       "│    └─Conv2d: 2-6                       [1, 128, 4, 4]            73,728\n",
       "│    └─BatchNorm2d: 2-7                  [1, 128, 4, 4]            256\n",
       "│    └─ReLU: 2-8                         [1, 128, 4, 4]            --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 4, 4]            147,456\n",
       "│    └─BatchNorm2d: 2-10                 [1, 128, 4, 4]            256\n",
       "│    └─ReLU: 2-11                        [1, 128, 4, 4]            --\n",
       "│    └─ReLU: 2-12                        [1, 128, 4, 4]            --\n",
       "├─ModuleList: 1-7                        --                        --\n",
       "│    └─Residual_Block: 2-13              [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-19                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-21                   [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-24                   [1, 128, 4, 4]            --\n",
       "│    └─Residual_Block: 2-14              [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-25                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-26            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-27                   [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-28                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-30                   [1, 128, 4, 4]            --\n",
       "│    └─Residual_Block: 2-15              [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-31                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-33                   [1, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-34                 [1, 128, 4, 4]            147,456\n",
       "│    │    └─BatchNorm2d: 3-35            [1, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-36                   [1, 128, 4, 4]            --\n",
       "├─Residual_Block_Transition: 1-8         [1, 256, 2, 2]            --\n",
       "│    └─Conv2d: 2-16                      [1, 256, 2, 2]            32,768\n",
       "│    └─BatchNorm2d: 2-17                 [1, 256, 2, 2]            512\n",
       "│    └─Conv2d: 2-18                      [1, 256, 2, 2]            294,912\n",
       "│    └─BatchNorm2d: 2-19                 [1, 256, 2, 2]            512\n",
       "│    └─ReLU: 2-20                        [1, 256, 2, 2]            --\n",
       "│    └─Conv2d: 2-21                      [1, 256, 2, 2]            589,824\n",
       "│    └─BatchNorm2d: 2-22                 [1, 256, 2, 2]            512\n",
       "│    └─ReLU: 2-23                        [1, 256, 2, 2]            --\n",
       "│    └─ReLU: 2-24                        [1, 256, 2, 2]            --\n",
       "├─ModuleList: 1-9                        --                        --\n",
       "│    └─Residual_Block: 2-25              [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-37                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-38            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-39                   [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-40                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-42                   [1, 256, 2, 2]            --\n",
       "│    └─Residual_Block: 2-26              [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-43                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-45                   [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-48                   [1, 256, 2, 2]            --\n",
       "│    └─Residual_Block: 2-27              [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-51                   [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-52                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-54                   [1, 256, 2, 2]            --\n",
       "│    └─Residual_Block: 2-28              [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-55                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-56            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-57                   [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-58                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-59            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-60                   [1, 256, 2, 2]            --\n",
       "│    └─Residual_Block: 2-29              [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-61                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-63                   [1, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-64                 [1, 256, 2, 2]            589,824\n",
       "│    │    └─BatchNorm2d: 3-65            [1, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-66                   [1, 256, 2, 2]            --\n",
       "├─Residual_Block_Transition: 1-10        [1, 512, 1, 1]            --\n",
       "│    └─Conv2d: 2-30                      [1, 512, 1, 1]            131,072\n",
       "│    └─BatchNorm2d: 2-31                 [1, 512, 1, 1]            1,024\n",
       "│    └─Conv2d: 2-32                      [1, 512, 1, 1]            1,179,648\n",
       "│    └─BatchNorm2d: 2-33                 [1, 512, 1, 1]            1,024\n",
       "│    └─ReLU: 2-34                        [1, 512, 1, 1]            --\n",
       "│    └─Conv2d: 2-35                      [1, 512, 1, 1]            2,359,296\n",
       "│    └─BatchNorm2d: 2-36                 [1, 512, 1, 1]            1,024\n",
       "│    └─ReLU: 2-37                        [1, 512, 1, 1]            --\n",
       "│    └─ReLU: 2-38                        [1, 512, 1, 1]            --\n",
       "├─ModuleList: 1-11                       --                        --\n",
       "│    └─Residual_Block: 2-39              [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-67                 [1, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-68            [1, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-69                   [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-70                 [1, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-71            [1, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-72                   [1, 512, 1, 1]            --\n",
       "│    └─Residual_Block: 2-40              [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-73                 [1, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-74            [1, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-75                   [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-76                 [1, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-78                   [1, 512, 1, 1]            --\n",
       "├─AdaptiveAvgPool2d: 1-12                [1, 512, 1, 1]            --\n",
       "├─Linear: 1-13                           [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 21,289,802\n",
       "Trainable params: 21,289,802\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 74.78\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.22\n",
       "Params size (MB): 85.16\n",
       "Estimated Total Size (MB): 86.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_base = ResNet(\n",
    "    num_repeats=[3, 4, 6, 3], in_channels=3, num_classes=10, first_out_channels=64\n",
    ").to(device)\n",
    "\n",
    "summary(resnet_base, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = resnet_base(random_tensor_batch)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook ends here as we are not training this model given the size of the model. We will try training a smaller model resnet-18 on cifar 10 dataset which we discuss in next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
